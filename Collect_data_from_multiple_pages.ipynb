{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzCgYpYN1mHi"
      },
      "source": [
        "Pour le scrapping des données, j'ai décidé d'utiliser `selenium`. Même s'il est moins rapide pour l'extraction de données simples, il fourni beaucoup plus de fonctionnalités, notamment l'extraction de données sur les sites qui utilisent du Javascript pour rendre les données dynamiquement sur le DOM (Document Object Model).\n",
        "\n",
        "**Note Additionnelle:**\n",
        "\n",
        "En raison de la lenteur constatée sur le scraping des données, nottement pour le Troisième lien qui  nécessite une requette aux pages individuelles des animeaux afin de pouvoir collecter les détails, `bs4` a été utilisé pour collecter les données depuis la page `HTML` récupérée par `driver.get(url)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a2Ve53RVxrYZ",
        "outputId": "74550e85-a46c-4971-aba4-182f3470c032"
      },
      "outputs": [],
      "source": [
        "# Installing selenium and bs4 in the notebook environment\n",
        "# !pip install selenium\n",
        "# !pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpCbeEEP24BK"
      },
      "outputs": [],
      "source": [
        "# Importing dependencies\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "gOR6SnnU03ZR",
        "outputId": "363e3d90-2d01-4999-aa9b-d132c99e233c"
      },
      "outputs": [],
      "source": [
        "# Setting up the environment (selenium driver options and get request)\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--headless\")  # The browser will run en detached mode (in the background)\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour le premier et les deux derniers liens, nous pouvons directement récupérer les informations, nottament:\n",
        "\n",
        "- le nom,\n",
        "- le prix,\n",
        "- l'adresse,\n",
        "- le lien vers l'image de l'animal (même si ce dernier conduit vers le thumbnail, qui est de moindre qualité).\n",
        "\n",
        "Par contre pour pour le troisième lien, nous devons impérativement envoyer une nouvelle requette HTTP à chacune des pages individuelles des animaux.\n",
        "Cette requette additionnelle aura une force incidence sur les performances de l'application, qui est proportionnelle au nombre de page a scraper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of page to be scraped for each of the link\n",
        "NUM_PAGE = 2\n",
        "\n",
        "# Website Base URL, will be used to reconstruct image URLs\n",
        "BASE_URL = \"https://sn.coinafrique.com\"\n",
        "\n",
        "# Creating a list of urls to be scraped\n",
        "ROOT_PAGE_LIST = [\n",
        "    {\n",
        "        \"title\": \"chiens\",\n",
        "        \"url\": \"https://sn.coinafrique.com/categorie/chiens?page=\",\n",
        "        \"keys\": (\"nom\", \"prix\", \"adresse\", \"image_lien\")\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"moutons\",\n",
        "        \"url\": \" https://sn.coinafrique.com/categorie/moutons?page=\",\n",
        "        \"keys\": (\"nom\", \"prix\", \"adresse\", \"image_lien\")\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"poules_lapins_et_pigeons\",\n",
        "        \"url\": \"https://sn.coinafrique.com/categorie/poules-lapins-et-pigeons?page=\",\n",
        "        \"keys\": (\"details\", \"prix\", \"adresse\", \"image_lien\"),\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"autres_animaux\",\n",
        "        \"url\": \"https://sn.coinafrique.com/categorie/autres-animaux?page=\",\n",
        "        \"keys\": (\"nom\", \"prix\", \"adresse\", \"image_lien\"),\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrap_simple(container):\n",
        "    v1 = container.find(\"p\", class_=\"ad__card-description\")\n",
        "    v2 = container.find(\"p\", class_=\"ad__card-price\")\n",
        "    v3 = container.find(\"p\", class_=\"ad__card-location\")\n",
        "    v4 = container.find(\"img\", class_=\"ad__card-img\")\n",
        "\n",
        "    return {\n",
        "        \"nom\": v1.a.text if v1 is not None else np.nan,\n",
        "        \"prix\": v2.a.text if v2 is not None else \"\",\n",
        "        \"adresse\": v3.span.text if v3 is not None else np.nan,\n",
        "        \"image_lien\": v4[\"src\"] if v4 is not None else np.nan\n",
        "    }\n",
        "\n",
        "\n",
        "def scrap_nested(path, base_url=None):\n",
        "    url = BASE_URL + path if base_url is None else base_url + path\n",
        "\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    soup = bs(driver.page_source, \"html.parser\")\n",
        "\n",
        "    v1 = soup.find(\"div\", class_=\"ad__info__box ad__info__box-descriptions\")\n",
        "    v2 = soup.find(\"p\", class_=\"price\")\n",
        "\n",
        "    v3 = soup.find(\"div\", class_=\"row valign-wrapper extra-info-ad-detail\")\n",
        "    v3 = v3 if (v3 is None) else v3.find_all(\"span\")\n",
        "    v3 = v3 if (v3 is None) else v3[2]\n",
        " \n",
        "    v4 = soup.find(id=\"slider\")\n",
        "    v4 = v4 if (v4 is None) else v4.find_all(\"div\")[0]\n",
        "\n",
        "    return {\n",
        "        \"details\": v1.find_all(\"p\")[-1].text if (v1 is not None) else np.nan,\n",
        "        \"prix\": v2.text if (v1 is not None) else \"\",\n",
        "        \"adresse\": v3.text if (v3 is not None) else np.nan,\n",
        "        \"image_lien\": v4[\"style\"].split('(')[1].split(')')[0].strip('\"') if (v4 is not None) else np.nan,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans la cellule suivante, une liste de tuple, contenant le nom de la page et une référence vers le DataFrame en mémoire a été créée. Le but est de s'en servir pour exporter les données (DataFrame vers une base de donnée sqlit3) dans une seule boucle, après avoir néttoyer les données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs: List[Tuple[str, pd.DataFrame]] = []  # [(\"DataFrame Name\", \"Reference to DataFrame\")]\n",
        "\n",
        "# Looping over ROOT_PAGE_LIST to scrap data from all URLs\n",
        "\n",
        "for page in ROOT_PAGE_LIST:\n",
        "    animal_data_list = []\n",
        "\n",
        "    for page_index in range(1, NUM_PAGE + 1):\n",
        "        page_url = f\"{page[\"url\"]}{page_index}\"\n",
        "        driver.get(page_url)\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = bs(driver.page_source, 'html.parser')\n",
        "        containers = soup.find_all(\"div\", class_=\"col s6 m4 l3\")\n",
        "\n",
        "        if page['title'] != \"poules_lapins_et_pigeons\":\n",
        "            animal_data_list.extend([scrap_simple(container) for container in containers])\n",
        "        else:\n",
        "            path_list = [container.find(\"p\", class_=\"ad__card-description\").a[\"href\"] for container in containers]\n",
        "\n",
        "            animal_data_list.extend([scrap_nested(path) for path in path_list])\n",
        "\n",
        "    dfs.append((page[\"title\"], pd.DataFrame(animal_data_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les informations que nous avons collecter sur la platforme, coinAfrique sont requisent par la modération pour qu'une annonce soit validée.\n",
        "Ceci dit, nous allons quand même les imputer pour éviter tout désagrément, au cas où le script manque à collecter une ou plusieurs données.\n",
        "\n",
        "Les prix, peuvent être `Sur Demande`, une chaîne de caractère. En convertissant cette colonne en numérique, ces valeurs seront transformées en NaN.\n",
        "\n",
        "Vu que ces valeurs manquantes pour `le prix` sont complètement aléatoires, nous pouvons les imputer:\n",
        "- par la moyenne si la distribution est symétrique et sans valeurs aberrantes\n",
        "- par la médiane si la distribution est asymétrique ou avec des valeurs aberrantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# plt.boxplot(dfs[0][1][\"prix\"], orientation=\"vertical\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# plt.hist(dfs[0][1][\"prix\"])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La méthode d'imputation approprié pour le prix, après visualisation est:\n",
        "- Une imputation simple par la mediane.\n",
        "\n",
        "Vu que les distributions sont asymétriques et avec des valeurs aberrantes.\n",
        "\n",
        "Pour les colonnes qualitatives:\n",
        "- `name`: la modalité la plus fréquente `mode`\n",
        "- `details`: une constante `Details Inconnu`\n",
        "- `adresse`: une constante `Adresse Inconnu`\n",
        "- `image_lien`: une constante `Lien Inconnu`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _, df in dfs:\n",
        "    df = df.drop_duplicates()\n",
        "    # prices with space in them, results to NaN, so we remove the spaces and the currencies\n",
        "    df[\"prix\"] = pd.to_numeric(df[\"prix\"].str.replace(\" \", \"\").str.rstrip(\"CFA\"), errors=\"coerce\")\n",
        "    # Replacing missing prices by the Median\n",
        "    df[\"prix\"] = df[\"prix\"].fillna(df[\"prix\"].median())\n",
        "\n",
        "    # Replacing missing adress and image_link with a constant\n",
        "    df['adresse'] = df['adresse'].fillna('Adresse Inconnue')\n",
        "    df['image_lien'] = df['image_lien'].fillna('Lien Inconnu')\n",
        "\n",
        "    # Replacing Name/Details with a constant\n",
        "    if \"nom\" in df.columns:\n",
        "        df[\"nom\"] = df[\"nom\"].fillna(df[\"nom\"].mode())\n",
        "    elif \"details\" in df.columns:\n",
        "        df[\"details\"] = df[\"details\"].fillna(\"Details Inconnus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exporting the data to a sqlite database\n",
        "\n",
        "# Create a connexion with a sqlite database (Animals.db)\n",
        "conn = sqlite3.connect('Animals.db')\n",
        "\n",
        "# Create a cursor (enabling interactions with the database)\n",
        "c = conn.cursor()\n",
        "\n",
        "# Create a table for each DataFrame and export them\n",
        "for title, df in dfs:\n",
        "    df.to_sql(name=title, con=conn, if_exists=\"replace\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
